import os
import numpy as np
from tqdm import tqdm

import torch
import torch.nn as nn

import utils.augmentations as aug
from utils.customdatasets import SegmentationLoader
from utils.metric import scores
from net_models import PSPNet, UNetWithResnet50Encoder

import pickle
import math
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

## Declare models
models = {
    'pspsqueezenet': lambda: PSPNet(sizes=(1, 2, 3, 6), psp_size=512, deep_features_size=256, backend='squeezenet'),
    'pspdensenet': lambda: PSPNet(sizes=(1, 2, 3, 6), psp_size=1024, deep_features_size=512, backend='densenet'),
    'pspresnet18': lambda: PSPNet(sizes=(1, 2, 3, 6), psp_size=512, deep_features_size=256, backend='resnet18'),
    'pspresnet34': lambda: PSPNet(sizes=(1, 2, 3, 6), psp_size=512, deep_features_size=256, backend='resnet34'),
    'pspresnet50': lambda: PSPNet(sizes=(1, 2, 3, 6), psp_size=2048, deep_features_size=1024, backend='resnet50'),
    'pspresnet101': lambda: PSPNet(sizes=(1, 2, 3, 6), psp_size=2048, deep_features_size=1024, backend='resnet101'),
    'pspresnet152': lambda: PSPNet(sizes=(1, 2, 3, 6), psp_size=2048, deep_features_size=1024, backend='resnet152'),
    'unetresnet50': lambda: UNetWithResnet50Encoder(n_classes=3)
}

# Building network
def build_network(snapshot, backend):
    epoch = 0
    backend = backend.lower()
    
    # Instantiate network model
    net = models[backend]()
    
    # Parallel training
    if torch.cuda.device_count() > 1:
        net = nn.DataParallel(net)
    
    # Load a pretrained network    
    if snapshot is not None:
        _, epoch = os.path.basename(snapshot).split('_')
        epoch = int(epoch)
        net.load_state_dict(torch.load(snapshot))
    
    # Sending model to GPU
    if torch.cuda.is_available():
        net = net.cuda()
        
    return net, epoch

def adjust_learning_rate(optimizer, epoch, opt):
    if opt.optimizer == 'sgd':
        lr_values = [ 0.01, 0.005, 0.001, 0.0005, 0.0001 ]
        step = round(opt.epochs / 5)

        idx = min(math.floor(epoch/step), len(lr_values))
        learning_rate = lr_values[idx]

        for param_group in optimizer.param_groups:
            param_group['lr'] = learning_rate

    return optimizer

def data_loader(split='train', batch_size=4):
    # Augmentations
    if split == 'train':
        augs = aug.Compose([aug.RandomRotate(10),
                            aug.RandomHorizontallyFlip(0.5),
                            aug.RandomVerticallyFlip(0.5),
                            aug.AdjustContrast(0.25),
                            aug.AdjustBrightness(0.25),
                            aug.AdjustSaturation(0.25)])
        shuffle = True
    else:
        augs = None
        shuffle = False

    dataset = SegmentationLoader(root='dataset/', augmentations=augs, split=split)
    loader = torch.utils.data.DataLoader(dataset=dataset, batch_size=batch_size, shuffle=shuffle)
    
    class_weight = torch.tensor([1., 1., 2.])
    if torch.cuda.is_available():
        class_weight = class_weight.cuda()

    if split != 'test':
        return loader, class_weight, len(dataset)
    else:
        return loader, dataset

def eval_metric(label_trues, label_preds, n_class):
    label_preds = torch.max(label_preds, 1)[1]
    
    if len(label_trues.shape) > 2:
        acc, miou = [], []
        for i in range(label_trues.shape[0]):
            score = scores(label_trues[i].cpu(), label_preds[i].cpu(), n_class)[0]
            miou.append(score['mean iou'])
            acc.append(score['overall acc'])
        return np.mean(miou), np.mean(acc) 
    else:
        score = scores(label_trues.cpu(), label_preds.cpu(), n_class)[0]
        return score['mean iou'], score['overall acc']
    
def scatterPlot(x_true, x_pred, output_name, figsize=(6,4.5), marker_color='g', curve_color='k'):
    reg = LinearRegression()
    reg.fit(x_true[:,None], x_pred[:,None])
    x_true_line = np.linspace(x_true.min(), x_true.max())
    x_pred_line = reg.predict(x_true_line[:,None])
    r2 = reg.score(x_true[:,None], x_pred[:,None])
    
    fig = plt.figure(figsize=figsize)
    #ax = fig.gca()
    
    title = 'RÂ²=:%.4f' % r2 
    
    plt.plot(x_true_line, x_pred_line, curve_color, alpha=0.7)
    plt.plot(x_true, x_pred, 'o%s' % marker_color, markersize=6, alpha=0.7)
    plt.grid()
    plt.xlabel('True severity')
    plt.ylabel('Predicted severity')
    plt.xlim(-0.01, 0.23)
    plt.ylim(-0.01, 0.24)
    plt.title(title)
    plt.show()
    fig.savefig('results/' + output_name + '.png', bbox_inches='tight', dpi=200)
    plt.close(fig)
    
# -------------------------------------------------------------------------------------- #
    
class SemanticSegmentation:
    def __init__(self, parser):
        self.opt = parser.parse_args()
        
    def train(self, train_loader, n_images, batch_size, epoch, model, seg_criterion, cls_criterion, optimizer, data_augmentation='mixup'):
        
        model.train()
        train_metrics = { 'loss': [], 'miou': [], 'acc': [] }
        
        train_iterator = tqdm(train_loader, total=n_images // batch_size + 1)
        for x, y, y_cls in train_iterator:
            
            # Loading images on gpu
            if torch.cuda.is_available():
                x, y, y_cls = x.cuda(), y.cuda(), y_cls.cuda()
                
            if data_augmentation == 'mixup':
                x, y_a, y_b, y_cls_a, y_cls_b, lam = aug.mixup_data(x, y, y_cls)
            
            # Pass images through the network
            out, out_cls = model(x)
            
            # Compute error
            if data_augmentation == 'mixup':
                seg_loss = aug.mixup_criterion(seg_criterion, out, y_a, y_b, lam)
                cls_loss = aug.mixup_criterion(cls_criterion, out_cls, y_cls_a, y_cls_b, lam)
            else:            
                seg_loss, cls_loss = seg_criterion(out, y), cls_criterion(out_cls, y_cls)
            
            loss = seg_loss + cls_loss
            
            # Clear gradients parameters
            model.zero_grad()            
            
            # Getting gradients
            loss.backward()
            
            # Updating parameters
            optimizer.step()
            
            # Metrics
            train_metrics['loss'].append(loss.data.cpu())
            
            if data_augmentation == 'mixup':
                metrics_a, metrics_b = eval_metric(y_a, out, 3), eval_metric(y_b, out, 3) 
                train_metrics['miou'].append(lam * metrics_a[0] + (1 - lam) * metrics_b[0])
                train_metrics['acc'].append(lam * metrics_a[1] + (1 - lam) * metrics_b[1])
            else:    
                metrics = eval_metric(y, out, 3)
                train_metrics['miou'].append(metrics[0])
                train_metrics['acc'].append(metrics[1])
            
            status = '[%i] loss = %.4f avg = %.4f, miou = %.4f, acc = %.4f' % (epoch + 1,
                                                                              loss.data.cpu(),
                                                                              np.mean(train_metrics['loss']),
                                                                              np.mean(train_metrics['miou']),
                                                                              np.mean(train_metrics['acc']))
            train_iterator.set_description(status)
        
        train_metrics['loss'] = np.mean(train_metrics['loss'])
        train_metrics['miou'] = np.mean(train_metrics['miou'])
        train_metrics['acc'] = np.mean(train_metrics['acc'])
        return train_metrics
    
    def validation(self, val_loader, n_images, batch_size, epoch, model, seg_criterion, cls_criterion):
        # tell to pytorch that we are evaluating the model
        model.eval()
    
        val_metrics = { 'loss': [], 'miou': [], 'acc': [] }
        
        val_iterator = tqdm(val_loader, total=n_images // batch_size + 1)            
        
        
        for x, y, y_cls in val_iterator:
            with torch.no_grad():
                # Loading images on gpu
                if torch.cuda.is_available():
	                x, y, y_cls = x.cuda(), y.cuda(), y_cls.cuda()

	            # pass images through the network
                out, out_cls = model(x)
                
                # Computer error
                seg_loss, cls_loss = seg_criterion(out, y), cls_criterion(out_cls, y_cls)
                loss = seg_loss + cls_loss

	            # Compute metrics
	            ## Loss
                # Metrics
                val_metrics['loss'].append(loss.data.cpu())
                metrics = eval_metric(y, out, 3)
                val_metrics['miou'].append(metrics[0])
                val_metrics['acc'].append(metrics[1])

                status = '[%i] loss = %.4f avg = %.4f, miou = %.4f, acc = %.4f' % (epoch + 1,
                                                                              loss.data.cpu(),
                                                                              np.mean(val_metrics['loss']),
                                                                              np.mean(val_metrics['miou']),
                                                                              np.mean(val_metrics['acc']))
                val_iterator.set_description(status)
        
        val_metrics['loss'] = np.mean(val_metrics['loss'])
        val_metrics['miou'] = np.mean(val_metrics['miou'])
        val_metrics['acc'] = np.mean(val_metrics['acc'])
        return val_metrics
      
    
    def run_training(self):
        # Dataset
        train_loader, class_weights, n_images_train = data_loader('train', self.opt.batch_size)
        val_loader, _, n_images_val = data_loader('val', self.opt.batch_size)
        
        # Model
        model, starting_epoch = build_network(self.opt.snapshot, self.opt.extractor)
        os.makedirs(os.path.abspath('net_weights'), exist_ok=True)

        # Criterion
        seg_criterion = nn.NLLLoss(weight=class_weights)
        cls_criterion = nn.BCEWithLogitsLoss(weight=class_weights)

        # Optimizer
        if self.opt.optimizer == 'sgd':
            optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=self.opt.weight_decay)
        else:
            optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=self.opt.weight_decay)

        record = {}
        record['extractor'] = self.opt.extractor
        record['batch_size'] = self.opt.batch_size
        record['weight_decay'] = self.opt.weight_decay
        record['optimizer'] = self.opt.optimizer
        record['epochs'] = self.opt.epochs
        record['train_loss'] = []
        record['val_loss'] = []
        record['train_miou'] = []
        record['val_miou'] = []
        record['train_acc'] = []
        record['val_acc'] = []
        
        os.makedirs(os.path.abspath('log'), exist_ok=True)

        best_loss = 1000.0
        
        for epoch in range(starting_epoch, starting_epoch + self.opt.epochs):
                
            # Training
            train_metrics = self.train(
                                    train_loader,
                                    n_images_train,
                                    self.opt.batch_size,
                                    epoch,
                                    model,
                                    seg_criterion,
                                    cls_criterion,
                                    optimizer,
                                    self.opt.data_augmentation
                                    )
            
            # Validation
            val_metrics = self.validation(
                                    val_loader,
                                    n_images_val,
                                    self.opt.batch_size,
                                    epoch,
                                    model,
                                    seg_criterion,
                                    cls_criterion
                                    )

            # Adjust learning rate
            optimizer = adjust_learning_rate(optimizer, epoch, self.opt)

            # Recording metrics
            record['train_loss'].append(train_metrics['loss'])
            record['train_miou'].append(train_metrics['miou'])
            record['train_acc'].append(train_metrics['acc'])

            record['val_loss'].append(val_metrics['loss'])
            record['val_miou'].append(val_metrics['miou'])
            record['val_acc'].append(val_metrics['acc'])

            # Record best model
            curr_loss = val_metrics['loss']
            if (curr_loss < best_loss):
                best_loss = curr_loss

                # Saving model
                torch.save(model, 'net_weights/' + self.opt.filename + '.pth')
                
                print('model saved')

            # Saving log
            fp = open('log/'+ self.opt.filename +'.pkl', 'wb')
            pickle.dump(record, fp)
            fp.close()

        # Plot
        # static_graph(np.array(record['train_dis_acc'])/100, np.array(record['val_dis_acc'])/100)

    def run_test(self):
        os.makedirs(os.path.abspath('results'), exist_ok=True)
        
        # Dataset
        test_loader, test_dataset = data_loader('test', self.opt.batch_size)

        # Loading model
        model = torch.load('net_weights/' + self.opt.filename + '.pth')
        model.cuda()
        
        # tell to pytorch that we are evaluating the model
        model.eval()
        
        test_metrics = { 'miou': [], 'acc': [] }
        severity = { 'true': np.array([]), 'pred': np.array([]) }

        with torch.no_grad():
            for imgs, labels, cls in test_loader:
                # Loading images on gpu
                if torch.cuda.is_available():
                    imgs, labels, cls = imgs.cuda(), labels.cuda(), cls.cuda()
        
                # pass images through the network
                out, out_cls = model(imgs)
                labels_pred = torch.max(out, 1)[1]
    
                # Compute metrics
                metrics = eval_metric(labels, out, 3)
                test_metrics['miou'].append(metrics[0])
                test_metrics['acc'].append(metrics[1])
                
                # Plot
                imgs = imgs.cpu().numpy()[:, ::-1, :, :]
                imgs = np.transpose(imgs, [0,2,3,1])
                
                f, axarr = plt.subplots(len(imgs), 3, figsize=(16, 11.5))
                
                for j in range(len(imgs)):
                    # Original image
                    axarr[j][0].imshow(imgs[j])
                    # True labels
                    axarr[j][1].imshow(test_dataset.decode_segmap(labels.cpu().numpy()[j]))
                    # Predicted labels
                    axarr[j][2].imshow(test_dataset.decode_segmap(labels_pred.cpu().numpy()[j]))
                
                    # Compute severity
                    aux = labels.cpu().numpy()[j]
                    severity['true'] = np.append(severity['true'], (aux==2).sum() / ((aux==1).sum() + (aux==2).sum()))
                    
                    aux = labels_pred.cpu().numpy()[j]
                    severity['pred'] = np.append(severity['pred'], (aux==2).sum() / ((aux==1).sum() + (aux==2).sum()))
                
                plt.setp(plt.gcf().get_axes(), xticks=[], yticks=[])
                plt.subplots_adjust(wspace=0.05, hspace=0.05)
                plt.show()

        miou = np.mean(test_metrics['miou'])
        acc = np.mean(test_metrics['acc'])

        f = open('results/' + self.opt.filename + '.csv', 'w')
        f.write('miou,acc\n%.2f,%.2f\n' % (miou*100, acc*100))
        f.close()
        
        print('miou,acc\n%.2f,%.2f\n' % (miou*100, acc*100))

        scatterPlot(severity['true'], severity['pred'], self.opt.filename)

    def get_n_params(self):
        model = torch.load('net_weights/' + self.opt.filename + '.pth')
        pp=0
        for p in list(model.parameters()):
            nn=1
            for s in list(p.size()):
                nn = nn*s
            pp += nn
        return pp
